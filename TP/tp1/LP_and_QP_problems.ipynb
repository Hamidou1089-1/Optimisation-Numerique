{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Fig/Ensimag.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>ENSIMAG  -- 2A</h3></center>\n",
    "<hr>\n",
    "<center><h1>Optimisation Num√©rique</h1></center>\n",
    "<center><h2>TP : Linear and Quadratic programs (1.5h)</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of an optimization program\n",
    "\n",
    "An optimization program can be practically divided into three parts:\n",
    "* the *run* environment, in which you test, run your program, and display results.\n",
    "* the *problem* part, which contains the function oracles, problem constraints, etc.\n",
    "* the *algorithmic* part, where the algorithms are coded.\n",
    "\n",
    "The main interest of such division is that these parts are interchangeable, meaning that, for instance, the algorithms of the third part can be used of a variety of problems. That is why such a decomposition is widely used.\n",
    "\n",
    "In the present lab, you will use this division:\n",
    "* `LP_and_QP_problems.ipynb` will be the *run* environment\n",
    "* `toy_problem.ipynb` and `imaging.ipynb` will be the considered *problems* for this lab\n",
    "* the library `cvxopt` will be used for solving all optimization problems\n",
    "\n",
    "---\n",
    "\n",
    "The following script will allow you to import *notebooks* as if you imported *python files* and will have to be executed at each time you launch Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T06:38:52.564143Z",
     "start_time": "2025-02-13T06:38:52.444683Z"
    }
   },
   "source": [
    "from statsmodels.genmod.families.links import identity\n",
    "\n",
    "import start\n",
    "from importlib import reload"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Regression model\n",
    "\n",
    "\n",
    "We consider the regression model\n",
    "\n",
    "$$ y=X\\theta+\\xi,\\;\\;\\xi\\sim \\mathcal{N}(0, \\sigma I_m), $$\n",
    "\n",
    "where $X\\in \\mathbb{R}^{m\\times n}$ and $y\\in \\mathbb{R}^m$ are the observed values and $\\theta\\in \\mathbb{R}^n$ is the unknown parameter we want to find. \n",
    "\n",
    "We want to find a value $\\widehat{\\theta}$ such that \n",
    "- the quadratic error $e(\\widehat{\\theta}) = 1/2 \\|X\\widehat{\\theta} - y \\|_2^2$ is (near)-minimal, that is $ \\| \\nabla e(\\widehat{\\theta}) \\| = \\| X^\\mathrm{T} (X\\widehat{\\theta} - y) \\| $ is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# the Dantzig Selector\n",
    "\n",
    "\n",
    "The **Dantzig Selector** for $\\theta$, introduced in *Emmanuel Candes and Terrence Tao \"The Dantzig selector: Statistical estimation when $p$ is much larger than $n$\". The Annals of Statistics, 2007* can be used to estimate $\\theta$ in the case of an overparametrized problem, i.e. when the dimension $n$ of $\\theta$ is well greater than the dimension of the observation $y$. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In that case, the estimator $\\widehat{\\theta}_{DS}$ is the solution of the optimization problem \n",
    "$$\n",
    "\\widehat{\\theta}_{DS} \\in \\arg\\min_{\\theta\\in \\mathbb{R}^n} \\left\\{\\|\\theta\\|_1,\\;\\mbox{with}\\;\\|X^T(X\\theta-y)\\|_\\infty\\leq \\kappa\\sigma\\right\\},\n",
    "$$\n",
    "where $\\kappa>0$ is an *hyper-parameter*. \n",
    "\n",
    "\n",
    "The best theoretical value for $\\kappa$ is $\\nu q_{\\mathcal{N}} \\left(1-\\frac{\\alpha}{2n}\\right)$, where  $\\alpha\\in(0,1)$ is the chosen risk level (e.g. $\\alpha=.05$), $q_\\mathcal{N}(p)$ is the $p$-quantile of the standard normal distribution, and $\\nu=\\max_j\\|[X]_j\\|_2$ is the maximal column norm of matrix $X$.\n",
    "\n",
    "\n",
    "**Objective:** Implement a function that return the Dantzig estimator $\\widehat{\\theta}_{DS}$ from input $(y,X,\\sigma)$ using linear programming solver `cvxopt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reformulate the $\\arg\\min$ problem as a linear program.\n",
    "\n",
    "\n",
    "> Write a function `DantzigSelector` that takes `y,X,sigma` as an input and outputs $\\widehat{\\theta}_{DS}$ by using the linear program solver of the library `cvxopt` <a href=\"http://cvxopt.org/userguide/coneprog.html#linear-programming\">lp</a> which, given $c, G, h$ solves the problem\n",
    "$$ \\min_x c^\\mathrm{T}x ~~~~~~ \\text{ subject to } Gx \\leq h $$\n",
    "where the inequality is elementwise.\n",
    "\n",
    "*Hint: Useful functions are* `np.concatenate` `np.zeros` `np.eye` `np.hstack` `np.vstack`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T10:09:50.756261Z",
     "start_time": "2025-02-13T10:09:50.747049Z"
    }
   },
   "source": [
    "import cvxopt\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "def DantzigSelector(y,X,sigma):\n",
    "    \n",
    "    # Extracting the sizes\n",
    "    m,n = X.shape\n",
    "    \n",
    "    # Computing kappa\n",
    "    alpha = 0.05\n",
    "    nu = max(np.linalg.norm(X, axis=0))\n",
    "    kappa = nu*norm.ppf(1-alpha/(2.0*n))\n",
    "\n",
    "    epsilon = kappa*sigma\n",
    "\n",
    "    X_transpose = np.transpose(X)\n",
    "    identity = np.eye(n)\n",
    "    matrix_zeros = np.zeros((n,n))\n",
    "    vector_zero = np.zeros(n)\n",
    "    vector_ones = np.ones(n)\n",
    "\n",
    "    X_t_X = np.dot(X_transpose, X)\n",
    "    X_t_Y = np.dot(X_transpose, y)\n",
    "\n",
    "    c = np.hstack((vector_ones, vector_ones))\n",
    "    h = np.hstack((X_t_Y + epsilon*vector_ones,-X_t_Y + epsilon*vector_ones,vector_zero, vector_zero))\n",
    "    G = np.vstack((\n",
    "        np.concatenate((X_t_X, -X_t_X), axis=1),\n",
    "        np.concatenate((-X_t_X, X_t_X), axis=1),\n",
    "        np.concatenate((-identity, matrix_zeros), axis=1),\n",
    "        np.concatenate((matrix_zeros, -identity), axis=1))\n",
    "    )\n",
    "\n",
    "    c = matrix(c)\n",
    "    G = matrix(G)\n",
    "    h = matrix(h)\n",
    "\n",
    "    theta = cvxopt.solvers.lp(c,G,h)\n",
    "    \n",
    "    return  theta"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T09:31:49.551095Z",
     "start_time": "2025-02-13T09:31:49.548248Z"
    }
   },
   "cell_type": "code",
   "source": "A",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> Test your code on the toy problem given in `toy_problem.ipynb`. For this randomly generated problem, there is a true $\\theta$ upon which the problem is built."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T10:09:54.640924Z",
     "start_time": "2025-02-13T10:09:54.128428Z"
    }
   },
   "source": [
    "import toy_problem as tPb\n",
    "reload(tPb)\n",
    "\n",
    "\n",
    "theta_ds = DantzigSelector(tPb.y,tPb.X,tPb.sigma)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres   k/t\n",
      " 0:  1.8031e-14 -2.1191e+02  2e+03  3e+00  1e+00  1e+00\n",
      " 1: -1.1207e+01 -2.7549e+01  8e+01  3e-01  8e-02  1e+00\n",
      " 2: -3.2959e+00 -1.0822e+01  3e+01  1e-01  4e-02  3e-01\n",
      " 3: -9.7364e-01 -5.7771e+00  2e+01  7e-02  2e-02  1e-01\n",
      " 4:  1.5807e+00 -2.7489e-01  8e+00  3e-02  9e-03  4e-02\n",
      " 5:  2.4866e+00  1.6201e+00  4e+00  1e-02  4e-03  2e-02\n",
      " 6:  3.1987e+00  3.1081e+00  4e-01  1e-03  4e-04  1e-03\n",
      " 7:  3.2855e+00  3.2842e+00  6e-03  2e-05  6e-06  2e-05\n",
      " 8:  3.2868e+00  3.2868e+00  6e-05  2e-07  6e-08  2e-07\n",
      " 9:  3.2869e+00  3.2869e+00  6e-07  2e-09  6e-10  2e-09\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Investigate the differences between the Dantzig selector and the Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T06:38:54.030594536Z",
     "start_time": "2025-02-08T22:11:41.213596Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(tPb.theta,'ro',label='ground truth')\n",
    "plt.plot(theta_ds,'b*',label='Dantzig selector')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T10:00:41.909322Z",
     "start_time": "2025-02-13T10:00:41.898361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "b = np.array([[4.1, 5.4, 6.7], [7.2, 8.2, 9.3], [10, 11, 12]])\n",
    "print(f\"Hsatack : \\n{np.hstack((a,b))}\")\n",
    "print(f\"Vstack: \\n{np.vstack((a,b))}\")\n",
    "print(f\"Concatenate: \\n{np.concatenate((a,b), axis=0)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hsatack : \n",
      "[[ 1.   2.   3.   4.1  5.4  6.7]\n",
      " [ 4.   5.   6.   7.2  8.2  9.3]\n",
      " [ 7.   8.   9.  10.  11.  12. ]]\n",
      "Vstack: \n",
      "[[ 1.   2.   3. ]\n",
      " [ 4.   5.   6. ]\n",
      " [ 7.   8.   9. ]\n",
      " [ 4.1  5.4  6.7]\n",
      " [ 7.2  8.2  9.3]\n",
      " [10.  11.  12. ]]\n",
      "Concatenate: \n",
      "[[ 1.   2.   3. ]\n",
      " [ 4.   5.   6. ]\n",
      " [ 7.   8.   9. ]\n",
      " [ 4.1  5.4  6.7]\n",
      " [ 7.2  8.2  9.3]\n",
      " [10.  11.  12. ]]\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# the Lasso\n",
    "\n",
    "Under the same regression model, the Least Absolute Shrinkage and Selection Operator or **lasso** for $\\theta$, introduced in *Robert Tibshirani \"Regression shrinkage and selection via the lasso\", Journal of the Royal Statistical Society, 1996* can also be used to estimate $\\theta$. \n",
    "\n",
    "The estimator $\\widehat{\\theta}_{L}$ is the solution of the optimization problem \n",
    "$$\n",
    "\\widehat{\\theta}_{L} \\in \\arg\\min_{\\theta\\in \\mathbb{R}^n} \\left\\{ \\|X\\theta - y\\|_2^2 + \\kappa \\sigma \\|\\theta\\|_1 \\right\\},\n",
    "$$\n",
    "where $\\kappa>0$ is an *hyper-parameter*. \n",
    "\n",
    "\n",
    "The best theoretical value for $\\kappa$ is the same as for the Dantzig selector: $\\nu q_{\\mathcal{N}} \\left(1-\\frac{\\alpha}{2n}\\right)$, where  $\\alpha\\in(0,1)$ is the chosen risk level (e.g. $\\alpha=.05$), $q_\\mathcal{N}(p)$ is the $p$-quantile of the standard normal distribution, and $\\nu=\\max_j\\|[X]_j\\|_2$ is the maximal column norm of matrix $X$.\n",
    "\n",
    "\n",
    "**Objective:** Implement a function that return the lasso estimator $\\widehat{\\theta}_{L}$ from input $(y,X,\\sigma)$ using quadratic programming solver `cvxopt`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reformulate the $\\arg\\min$ problem as a quadratic program.\n",
    "\n",
    "\n",
    "> Write a function `Lasso` that takes `y,X,sigma` as an input and outputs $\\widehat{\\theta}_{L}$ by using the quadratic program solver of the library `cvxopt` <a href=\"http://cvxopt.org/userguide/coneprog.html#quadratic-programming\">qp</a> which, given $P,q, G, h$ solves the problem\n",
    "$$ \\min_x 1/2 x^\\mathrm{T} P x + q^\\mathrm{T}x ~~~~~~ \\text{ subject to } Gx \\leq h $$\n",
    "where the inequality is elementwise.\n",
    "\n",
    "*Hint: Useful functions are* `np.concatenate` `np.zeros` `np.eye` `np.hstack` `np.vstack`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix, solvers\n",
    "\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "def Lasso(y,X,sigma):\n",
    "    \n",
    "    # Extracting the sizes\n",
    "    m,n = X.shape\n",
    "    \n",
    "    # Computing kappa\n",
    "    alpha = 0.05\n",
    "    nu = max(np.linalg.norm(X, axis=0))\n",
    "    kappa = nu*norm.ppf(1-alpha/(2.0*n)) \n",
    "    \n",
    "    #####################################################\n",
    "    # COMPUTE AND SOLVE QP PROBLEM\n",
    "    #####################################################\n",
    "\n",
    "    theta = np.zeros(n)  \n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> Test your code on the toy problem given in `toy_problem.ipynb`. For this randomly generated problem, there is a true $\\theta$ upon which the problem is built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toy_problem as tPb\n",
    "reload(tPb)\n",
    "\n",
    "\n",
    "theta_l = Lasso(tPb.y,tPb.X,tPb.sigma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Investigate the differences between the Lasso and the Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(tPb.theta,'ro',label='ground truth')\n",
    "plt.plot(theta_l,'k*',label='Lasso')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing and Improving\n",
    "\n",
    "\n",
    "> Compute the lasso and Dantzig selector solutions for the same problem and investigate their compared performance graphically and by computing the error on the null and non-null coordinates of theta.\n",
    "\n",
    "\n",
    "> Play with the values of $n,m,\\sigma, S$ to exhibit differences in behaviors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toy_problem as tPb\n",
    "reload(tPb)\n",
    "\n",
    "\n",
    "theta_l = Lasso(tPb.y,tPb.X,tPb.sigma)\n",
    "theta_ds = DantzigSelector(tPb.y,tPb.X,tPb.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(tPb.theta,'ro',label='ground truth')\n",
    "plt.plot(theta_ds,'b*',label='Dantzig selector')\n",
    "plt.plot(theta_l,'k*',label='Lasso')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(tPb.theta[tPb.non_null],'ro',label='ground truth')\n",
    "plt.plot(theta_ds[tPb.non_null],'b*',label='Dantzig selector')\n",
    "plt.plot(theta_l[tPb.non_null],'k*',label='Lasso')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "err_theta_DS = np.linalg.norm(theta_ds[tPb.non_null] -tPb.theta[tPb.non_null]  , ord=1)\n",
    "err_theta_L = np.linalg.norm(theta_l[tPb.non_null]-tPb.theta[tPb.non_null] , ord=1)\n",
    "\n",
    "print(\"Error on the non-null coefficients\\n Dantzig selec. \\t Lasso\")\n",
    "print(err_theta_DS,err_theta_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_supp_DS = 0\n",
    "err_supp_L = 0\n",
    "for i in range(tPb.n):\n",
    "    if i not in  tPb.non_null:\n",
    "        err_supp_DS += np.abs(theta_ds[i])\n",
    "        err_supp_L += np.abs(theta_l[i])\n",
    "\n",
    "print(\"Error on the null coefficients\\n Dantzig selec. \\t Lasso\")\n",
    "print(float(err_supp_DS),float(err_supp_L))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error on y \\n Dantzig selec. \\t Lasso\")\n",
    "print(float(np.linalg.norm(np.dot(tPb.X, theta_ds ) - tPb.y )),float(np.linalg.norm(np.dot(tPb.X, theta_l ) - tPb.y )))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quadratic error \\n Dantzig selec. \\t Lasso\")\n",
    "print(float(np.linalg.norm(np.dot(tPb.X.T , np.dot(tPb.X, theta_ds ) - tPb.y ))),float(np.linalg.norm(np.dot(tPb.X.T , np.dot(tPb.X, theta_l ) - tPb.y )))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# To go further\n",
    "\n",
    "\n",
    "### Improved Estimators\n",
    "\n",
    "A popular improvement of these estimators is to weigh the threshold $\\kappa$ with the norm corresponding column of $X$.\n",
    "\n",
    "The improved Dantzig and Lasso estimators $\\widehat{\\theta'}_{DS}$ and $\\widehat{\\theta'}_{L}$  are thus solutions of  \n",
    "$$\n",
    "\\widehat{\\theta}_{DS} \\in \\arg\\min_{\\theta\\in \\mathbb{R}^n} \\left\\{\\|\\theta\\|_1,\\;\\mbox{with}\\;| [ X^T(X\\theta-y)]_i  | \\leq \\kappa_i \\sigma ~~ \\forall i\\right\\},\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\widehat{\\theta}_{L} \\in \\arg\\min_{\\theta\\in \\mathbb{R}^n} \\left\\{ \\|X\\theta - y\\|_2^2 + \\sum_{i=1}^n \\kappa_i \\sigma |\\theta_i | \\right\\},\n",
    "$$\n",
    "where $\\kappa_i = \\| X_i \\|_2 \\kappa = \\| X_i \\|_2 \\nu q_{\\mathcal{N}} \\left(1-\\frac{\\alpha}{2n}\\right) $ where $\\| X_i \\|_2$ is the $2$-norm of the $i$-th column of $X$.\n",
    "\n",
    "\n",
    "> Investigate the pertinence of this improvement. (Get rid of the normalization in the problem). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossed Validation of the thresholds\n",
    "\n",
    "The values of $\\kappa$ is heavily linked to the noise standard deviation $\\sigma$ and theoretical considerations about $X$ and $\\theta$. A practical way to choose $\\kappa$ without the knowledge of $\\sigma$ is to use *cross-validation*.\n",
    "\n",
    "- Split y and X in two parts column-wise, one part for *training* and one for *testing*.\n",
    "- For several values of $\\kappa$, solve the estimation problem *on the training part* using $\\kappa$ to get a estimate $\\widehat{\\theta}_\\kappa$.\n",
    "- For each $\\widehat{\\theta}_\\kappa$, compute the error *on the testing part* $e(\\kappa) = \\| y_{test}-X_{test}\\widehat{\\theta}_\\kappa \\|$.  \n",
    "- Choose the value that minimize this error $\\widehat{\\kappa} = \\arg \\min_{\\kappa} e(\\kappa)$.\n",
    "\n",
    "\n",
    "> Implement a cross validation procedure to choose $\\kappa$. Compare the value and the outputted estimate with the theoretic value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
